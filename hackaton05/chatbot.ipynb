{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                CHATBOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Updated imports based on LangChain v0.2+\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "productos = [\n",
    "    {\n",
    "        \"nombre\": \"iPhone 14 Pro\",\n",
    "        \"descripcion\": \"Pantalla OLED de 6.1 pulgadas, chip A16 Bionic, cámara de 48 MP.\",\n",
    "        \"precio\": 1099.99,\n",
    "        \"cantidad\": 5\n",
    "    },\n",
    "    {\n",
    "        \"nombre\": \"Samsung Galaxy S23\",\n",
    "        \"descripcion\": \"Pantalla Dynamic AMOLED de 6.8 pulgadas, Snapdragon 8 Gen 2, cámara de 200 MP.\",\n",
    "        \"precio\": 999.99,\n",
    "        \"cantidad\": 8\n",
    "    },\n",
    "    {\n",
    "        \"nombre\": \"Xiaomi 13 Pro\",\n",
    "        \"descripcion\": \"Pantalla AMOLED de 6.73 pulgadas, Snapdragon 8 Gen 2, cámara Leica de 50 MP.\",\n",
    "        \"precio\": 899.99,\n",
    "        \"cantidad\": 10\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark1/Documents/IDAT_IA/hackaton05/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "# 1.- COnfiguramos del embedding\n",
    "embedings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.- convertir las descripciones de los productos en vectores, embeddings y almacenarlos en un vector store\n",
    "\n",
    "doc = [Document(page_content= producto[\"descripcion\"] , metadata={\"nombre\": producto[\"nombre\"], \"precio\": producto[\"precio\"], \"cantidad\": producto[\"cantidad\"]}) for producto in productos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/d3vm4xm12xnf9r5q7yhtsl6w0000gn/T/ipykernel_4749/3702246971.py:3: RuntimeWarning: coroutine 'VectorStore.aadd_documents' was never awaited\n",
      "  vectorstore = Chroma.from_documents(doc, embedings)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "# 3.- crear y guardar el vector dentro de una bd \n",
    "vectorstore = Chroma.from_documents(doc, embedings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/d3vm4xm12xnf9r5q7yhtsl6w0000gn/T/ipykernel_4749/1971932241.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# 4.- configuracion de la memoria para mantener la conversacion\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    input_key= \"question\",\n",
    "    output_key= \"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.- crea ek sistema de recuperacion conversacional con RAG\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    verbose = True,\n",
    "    output_key = \"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa: memory=ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), output_key='answer', input_key='question', return_messages=True, memory_key='chat_history') verbose=False combine_docs_chain=StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x176f82950>, async_client=<openai.resources.completions.AsyncCompletions object at 0x176f7c450>, temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy='', logit_bias={}), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x176f82950>, async_client=<openai.resources.completions.AsyncCompletions object at 0x176f7c450>, temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy='', logit_bias={}), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x16ea48e50>, search_kwargs={'k': 3})\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Pantalla Dynamic AMOLED de 6.8 pulgadas, Snapdragon 8 Gen 2, cámara de 200 MP.\n",
      "\n",
      "Pantalla AMOLED de 6.73 pulgadas, Snapdragon 8 Gen 2, cámara Leica de 50 MP.\n",
      "\n",
      "Pantalla OLED de 6.1 pulgadas, chip A16 Bionic, cámara de 48 MP.\n",
      "\n",
      "Question: ¿Cuál es el precio del iPhone 14 Pro?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "result: [Document(metadata={'cantidad': 8, 'nombre': 'Samsung Galaxy S23', 'precio': 999.99}, page_content='Pantalla Dynamic AMOLED de 6.8 pulgadas, Snapdragon 8 Gen 2, cámara de 200 MP.'), Document(metadata={'cantidad': 10, 'nombre': 'Xiaomi 13 Pro', 'precio': 899.99}, page_content='Pantalla AMOLED de 6.73 pulgadas, Snapdragon 8 Gen 2, cámara Leica de 50 MP.'), Document(metadata={'cantidad': 5, 'nombre': 'iPhone 14 Pro', 'precio': 1099.99}, page_content='Pantalla OLED de 6.1 pulgadas, chip A16 Bionic, cámara de 48 MP.')]\n",
      " No puedo determinar el precio del iPhone 14 Pro ya que no se menciona en la información proporcionada. \n"
     ]
    }
   ],
   "source": [
    "def chat_with_bot(query):\n",
    "    print('qa:', qa)\n",
    "    result = qa.invoke({\"question\": query})\n",
    "\n",
    "    print(\"result:\", result.get(\"source_documents\", []))\n",
    "\n",
    "    if not result.get(\"source_documents\"):\n",
    "        return \"No tengo respuesta para esa pregunta\"\n",
    "    \n",
    "    return result[\"answer\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(chat_with_bot(\"¿Cuál es el precio del iPhone 14 Pro?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
